{"cells":[{"source":"# Joining Data with Pandas","metadata":{},"cell_type":"markdown","id":"500aa129-0f1b-4c33-9fb9-34236f6897fe"},{"source":"## 0 - Setup Environment","metadata":{},"cell_type":"markdown","id":"1aea5ad9-6958-45b7-8a24-59430cdef53e"},{"source":"from matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd","metadata":{},"cell_type":"code","id":"cb95cba2-688f-447d-9146-1cb17d0781bb","execution_count":null,"outputs":[]},{"source":"## 1 - Data Merging Basics","metadata":{"tags":[]},"cell_type":"markdown","id":"debfd4d9-24bd-4274-a581-6a0383e258be"},{"source":"### Inner Joins | DF Loads: taxi_owners, taxi_veh, census, wards, biz_owners, licenses","metadata":{"tags":[]},"cell_type":"markdown","id":"d515849c-dae6-494f-88f8-cd1441d6085c"},{"source":"##### Merge the taxi_owners and taxi_veh tables setting a suffix for columns w/ the same names","metadata":{},"cell_type":"markdown","id":"b7e98a5a-a0d6-430d-b4d6-42c9576d72e2"},{"source":"taxi_owners = pd.read_pickle(r'datasets/taxi_owners.p')\ntaxi_veh = pd.read_pickle(r'datasets/taxi_vehicles.p')\n\ntaxi_own_veh = taxi_owners.merge(taxi_veh, on='vid', suffixes=('_own', '_veh'))\nprint(taxi_own_veh.columns)","metadata":{"tags":[]},"cell_type":"code","id":"55cca970-2174-4bfb-a980-1e30a1bc0432","execution_count":null,"outputs":[]},{"source":"##### Inner joins and number of rows returned","metadata":{},"cell_type":"markdown","id":"66813524-e103-467c-832e-afc1e04858af"},{"source":"census = pd.read_pickle(r'datasets/census.p')\nwards = pd.read_pickle(r'datasets/ward.p')\n\nwards_census = wards.merge(census, on='ward')\nprint(f'wards_census table shape: {wards_census.shape}')","metadata":{},"cell_type":"code","id":"74efc77c-dd1c-4f44-8eab-45996768df5a","execution_count":null,"outputs":[]},{"source":"##### One-to-many merge","metadata":{},"cell_type":"markdown","id":"1c1afdc0-f908-4e86-83b5-02d68b28c4c2"},{"source":"biz_owners = pd.read_pickle(r'datasets/business_owners.p')\nlicenses = pd.read_pickle(r'datasets/licenses.p')\n\nlicenses_owners = licenses.merge(biz_owners, on = 'account')\n\n# Group the results by title then count the number of accounts\n# NOTE: a dict {columm:f(x)}, i.e., {'account':'count'}, can be passed to .agg(...) to indicate different f(x)s for different columns\ncounted_df = licenses_owners.groupby('title').agg({'account':'count'})\nsorted_df = counted_df.sort_values('account', ascending = False)\nprint(sorted_df.head())","metadata":{"tags":[]},"cell_type":"code","id":"0d8d4fc3-c8ec-4a44-b1c1-6709f4ea1a31","execution_count":null,"outputs":[]},{"source":"### Merging Multiple DataFrames | DFs Loads: cal, ridership, stations, land_use","metadata":{"tags":[]},"cell_type":"markdown","id":"cff25f26-b3c6-412f-abcb-6f190e5b4552"},{"source":"##### Merge 3 DataFrames and Create Complex Filter Criteria","metadata":{},"cell_type":"markdown","id":"1ba214ee-1211-445b-9d4f-1180382de09b"},{"source":"cal = pd.read_pickle(r'datasets/cta_calendar.p')\nridership = pd.read_pickle(r'datasets/cta_ridership.p')\nstations = pd.read_pickle(r'datasets/stations.p')\n\nridership_cal_stations = ridership.merge(cal, on=['year','month','day']).merge(stations, on='station_id')\nfilter_criteria = ((ridership_cal_stations['month'] == 7) \n                   & (ridership_cal_stations['day_type'] == 'Weekday') \n                   & (ridership_cal_stations['station_name'] == 'Wilson'))\nprint(ridership_cal_stations.loc[filter_criteria, 'rides'].sum())","metadata":{},"cell_type":"code","id":"c07fe682-8da8-4f5b-87d4-602c88a951ce","execution_count":null,"outputs":[]},{"source":"##### One-to-Many Merge with Multiple DataFrames - 'SQL-like' Results, i.e., columns passed to .groupby(...) are NOT converted to a MultIndex/Index","metadata":{},"cell_type":"markdown","id":"33679ad9-aeb6-4aee-ac1d-274462ff5719"},{"source":"land_use = pd.read_pickle(r'datasets/land_use.p')\n\nland_cen_lic = land_use.merge(census, on='ward').merge(licenses, on='ward', suffixes=('_cen','_lic'))\n# as_index = False is responsible for the 'SQL-like' results\npop_vac_lic = land_cen_lic.groupby(['ward', 'pop_2010', 'vacant'], as_index = False).agg({'account':'count'})\nsorted_pop_vac_lic = pop_vac_lic.sort_values(['vacant', 'account', 'pop_2010'], ascending = [False, True, True])\nprint(sorted_pop_vac_lic.head())","metadata":{},"cell_type":"code","id":"d314755f-a5af-4d06-8019-ce87cf34064f","execution_count":null,"outputs":[]},{"source":"## 2 - Merging Tables with Different Join Types","metadata":{"tags":[]},"cell_type":"markdown","id":"dde87b65-0ecc-44a5-97d4-b6e053253a65"},{"source":"### Joins | DF Loads: financials, movies, taglines, movie_to_genres","metadata":{"tags":[]},"cell_type":"markdown","id":"a02cc399-2f68-4425-9d23-969f15b15098"},{"source":"##### Counting Missing Rows with Left Join","metadata":{},"cell_type":"markdown","id":"93363779-ae9b-47d8-aaa1-a594ad46a1f0"},{"source":"financials = pd.read_pickle(r'datasets/financials.p')\nmovies = pd.read_pickle(r'datasets/movies.p')\n\nmovies_financials = movies.merge(financials, on='id', how='left')\n\n# using .sum(), count the number of NULLS in the budget column; using .count() is equivalent to select count(*)... w/o a where clause\nnumber_of_missing_fin = movies_financials['budget'].isnull().sum()\nprint(number_of_missing_fin)","metadata":{"tags":[]},"cell_type":"code","id":"29dceeed-619c-4ac2-8cca-1cfc47bfd16c","execution_count":4,"outputs":[]},{"source":"##### Enriching a dataset","metadata":{},"cell_type":"markdown","id":"fdf40db6-eec8-44af-9221-495dbf8e87f4"},{"source":"taglines = pd.read_pickle(r'datasets/taglines.p')\n\n# notice that the .str accessor MUST be called multiple times; 1) .title.str.lower() 2) .title.str.lower().str.contains(...)\ntoy_story = movies[movies.title.str.lower().str.contains('toy story')]\n\nprint('========== Left Join Results ==========')\ntoystory_tag_lj = toy_story.merge(taglines, how = 'left', on = 'id')\nprint(toystory_tag_lj)\n\nprint('========== Inner Join Results ==========')\ntoystory_tag_ij = toy_story.merge(taglines, how = 'inner', on = 'id')\nprint(toystory_tag_ij)","metadata":{},"cell_type":"code","id":"c3fcc8f2-32eb-4994-bdf1-c145ed693253","execution_count":null,"outputs":[]},{"source":"##### Right join to Find Sci-Fi Movies","metadata":{},"cell_type":"markdown","id":"c019cb9c-c0df-482d-9774-68fd2ca59f07"},{"source":"movie_to_genres = pd.read_pickle(r'datasets/movie_to_genres.p')\n# attempted to re-create dataset not included w/ course material; new method saves DS to CSV, then prints it to screen to be copied\n# scifi_movies = movie_to_genres.query('genre == \"Science Fiction\"').merge(movies, left_on = 'movie_id', right_on = 'id')[movie_to_genres.columns]\n# action_movies = movie_to_genres.query('genre == \"Action\"').merge(movies, left_on = 'movie_id', right_on = 'id')[movie_to_genres.columns]\nscifi_movies = pd.read_csv(r'datasets/scifi_movies.csv')\naction_movies = pd.read_csv(r'datasets/action_movies.csv')\naction_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right',suffixes = ['_act', '_sci'])\n\nscifi_only = action_scifi[action_scifi['genre_act'].isnull()]\nmovies_and_scifi_only = movies.merge(scifi_only, left_on = 'id', right_on ='movie_id')\n\nprint(movies_and_scifi_only.head())\nprint(movies_and_scifi_only.shape)","metadata":{},"cell_type":"code","id":"2c8c0e03-2459-4cd2-bde8-4919dc646884","execution_count":5,"outputs":[]},{"source":"##### Popular Genres with Right Join","metadata":{},"cell_type":"markdown","id":"fa498d9f-0ab4-4c29-beb2-a9bc96eaad25"},{"source":"# top 10 most popular movies\npop_movies = movies.sort_values('popularity', ascending = False).head(10)\ngenres_movies = movie_to_genres.merge(pop_movies, how='right', left_on = 'movie_id', right_on = 'id')\ngenre_count = genres_movies.groupby('genre').agg({'id':'count'})\n\ngenre_count.plot(kind='bar')\nplt.show()","metadata":{},"cell_type":"code","id":"0f48f89c-27e7-425d-bb28-f5d5a6c9bf66","execution_count":null,"outputs":[]},{"source":"### Index merge for movie ratings | DF Loads: ratings","metadata":{"tags":[]},"cell_type":"markdown","id":"5c650b47-69d6-4544-b609-1bec350f9992"},{"source":"ratings = pd.read_pickle(r'datasets/ratings.p')\n\nratings.set_index('id', inplace = True)\nmovies.set_index('id', inplace = True)\nmovies_ratings = movies.merge(ratings, on = 'id', how = 'left')\n\nprint(movies_ratings.head())","metadata":{},"cell_type":"code","id":"22a622a4-03e1-4b98-99fe-1246fdcaac59","execution_count":null,"outputs":[]},{"source":"## 3 - Advance Merging and Concatenation","metadata":{"tags":[]},"cell_type":"markdown","id":"01a0e530-5da3-4121-a29e-b90c7412b286"},{"source":"### Advanced Joins","metadata":{},"cell_type":"markdown","id":"ca8bbae5-7aaa-4213-b629-da782ff5fdef"},{"source":"##### Semi-Join, i.e., Inner Join 2 DFs, Keeping Only UNIQUE Rows from Left DF","metadata":{"tags":[],"jp-MarkdownHeadingCollapsed":true},"cell_type":"markdown","id":"65d2a7f6-d55e-479f-9ac9-c49466e8f9d7"},{"source":"# attempted to re-create dataset not included w/ course material; new method saves DS to CSV, then prints it to screen to be copied\nfrom sqlite3_conn_mgr import sqlite3_conn_mgr\n\nwith sqlite3_conn_mgr(r'datasets/chinook.db') as conn:\n    genres = pd.read_sql_query('select * from genres', conn)\n    # top_10_tracks = pd.read_sql_query('select t.* from tracks t inner join invoice_items i on i.TrackId = t.TrackId order by i.Quantity desc limit 10', conn)\n    top_10_tracks = pd.read_sql_query('select * from tracks where TrackId <= 10', conn)\n\n# print(f'{\"=\"*10} genres.info() {\"=\"*10}')\n# genres.info()\n# print(f'{\"=\"*10} tracks.info() {\"=\"*10}')\n# top_10_tracks.info()\n\ngenres_tracks = genres.merge(top_10_tracks, on = 'GenreId')\nprint(genres_tracks.head())\n# semi-join returns only matching rows from the left DF\ntop_genres = genres[genres.GenreId.isin(genres_tracks.GenreId)]\nprint(top_genres.head())","metadata":{},"cell_type":"code","id":"77163b5f-63a8-48ad-be3f-de11ff1e2a4a","execution_count":null,"outputs":[]},{"source":"##### Anti-Join, i.e., Left Join 2 DFs, Keeping ...","metadata":{"tags":[],"jp-MarkdownHeadingCollapsed":true},"cell_type":"markdown","id":"5717fbe0-af70-4562-aa2f-62b45f7c1d5e"},{"source":"# attempted to re-create dataset not included w/ course material; new method saves DS to CSV, then prints it to screen to be copied\n# with sqlite3_conn_mgr(r'datasets/chinook.db') as conn:\n#     employees = pd.read_sql_query('select * from employees', conn)\n#     # top_10_tracks = pd.read_sql_query('select t.* from tracks t inner join invoice_items i on i.TrackId = t.TrackId order by i.Quantity desc limit 10', conn)\n#     top_cust = pd.read_sql_query('select * from customers where CustomerId < 60', conn)\n\n# empl_cust = employees.merge(top_cust, how = 'left', left_on = 'EmployeeId', right_on = 'SupportRepId', indicator = True)\n# srid_list = empl_cust.loc[empl_cust._merge == 'left_only', 'SupportRepId']\n# print(employees[employees.EmployeeId.isin(srid_list)])\n\nemployees = pd.read_csv(r'datasets/employees.csv')\ntop_cust = pd.read_csv(r'datasets/top_cust.csv')\nempl_cust = employees.merge(top_cust, on = 'srid', how = 'left', indicator = True)\n# Select the srid column where _merge is left_only\nsrid_list = empl_cust.loc[empl_cust['_merge'] == 'left_only', 'srid']\n\n# Get employees not working with top customers\nprint(employees[employees.srid.isin(srid_list)])","metadata":{},"cell_type":"code","id":"bce386c0-3bc9-4148-9789-a01b583d23c0","execution_count":null,"outputs":[]},{"source":"##### Semi-Join","metadata":{},"cell_type":"markdown","id":"134e3e06-b6ec-437a-9410-857f57631b6d"},{"source":"non_mus_tcks = pd.read_csv(r'datasets/non_mus_tcks.csv')\ntop_invoices = pd.read_csv(r'datasets/top_invoices.csv')\ngenres = pd.read_csv(r'datasets/genres.csv')\n\n# Merge the non_mus_tck and top_invoices tables on tid\ntracks_invoices = non_mus_tcks.merge(top_invoices, on = 'tid')\n\n# Use .isin() to subset non_mus_tcks to rows with tid in tracks_invoices\ntop_tracks = non_mus_tcks[non_mus_tcks['tid'].isin(tracks_invoices.tid)]\n\n# Group the top_tracks by gid and count the tid rows\ncnt_by_gid = top_tracks.groupby(['gid'], as_index=False).agg({'tid':'count'})\n\n# Merge the genres table to cnt_by_gid on gid and print\nprint(cnt_by_gid.merge(genres, on = 'gid'))","metadata":{},"cell_type":"code","id":"26d3c4a9-86ec-445f-a65f-c6da9b54ee36","execution_count":null,"outputs":[]},{"source":"### Concatenating DataFrames","metadata":{},"cell_type":"markdown","id":"5783aa6f-2659-48a6-90a5-8ef7e72f8a8c"},{"source":"##### Vertical Concatenation","metadata":{},"cell_type":"markdown","id":"082e617f-06d5-425d-a49d-02df6c39f699"},{"source":"tracks_master = pd.read_csv(r'datasets/tracks_master.csv')\ntracks_ride = pd.read_csv(r'datasets/tracks_ride.csv')\ntracks_st = pd.read_csv(r'datasets/tracks_st.csv')\n\ntracks_from_albums = pd.concat([tracks_master, tracks_ride, tracks_st], sort=True)\nprint(tracks_from_albums)","metadata":{},"cell_type":"code","id":"a1d15aa0-06ff-42a3-bb26-64ca99b70727","execution_count":null,"outputs":[]},{"source":"##### Concatenate, Ignoring Existing Indexes","metadata":{},"cell_type":"markdown","id":"f2733b80-8659-4b4e-a5c8-8fecf8c9df60"},{"source":"# Concatenate the tracks so the index goes from 0 to n-1\ntracks_from_albums = pd.concat([tracks_master, tracks_ride, tracks_st], ignore_index = True, sort = True)\nprint(tracks_from_albums)","metadata":{},"cell_type":"code","id":"4f66a7e1-a6e4-4cd0-9f7e-ce0f7033a6bb","execution_count":null,"outputs":[]},{"source":"##### Concatenate DFs with Different Columns - Keep Only Common Columns","metadata":{"tags":[]},"cell_type":"markdown","id":"bf7081c6-b4c6-4294-8219-1d84a48af92e"},{"source":"# Concatenate the tracks, show only columns names that are in all tables\ntracks_from_albums = pd.concat([tracks_master, tracks_ride, tracks_st], join = 'inner', sort = True)\nprint(tracks_from_albums)","metadata":{},"cell_type":"code","id":"e419b401-ac20-4e0f-993d-b369c1fd4aeb","execution_count":null,"outputs":[]},{"source":"##### Concatenating with Keys","metadata":{},"cell_type":"markdown","id":"5f53ea96-9d22-474c-83ae-b516b8337afb"},{"source":"inv_jul = pd.read_csv(r'datasets/inv_jul.csv')\ninv_aug = pd.read_csv(r'datasets/inv_aug.csv')\ninv_sep = pd.read_csv(r'datasets/inv_sep.csv')\n\n# Concatenate the tables and add keys\ninv_jul_thr_sep = pd.concat([inv_jul, inv_aug, inv_sep], keys=['7Jul', '8Aug', '9Sep'])\n\n# Group the invoices by the index keys and find avg of the total column\navg_inv_by_month = inv_jul_thr_sep.groupby(level=0).agg({'total': 'mean'})\n\n# Bar plot of avg_inv_by_month\navg_inv_by_month.plot(kind = 'bar')\nplt.show()","metadata":{},"cell_type":"code","id":"9d82a3ab-a906-4992-beba-71509ad439da","execution_count":null,"outputs":[]},{"source":"##### Concatenate and merge to find common songs","metadata":{},"cell_type":"markdown","id":"61bf3576-00ca-449e-862d-9c4edc01c843"},{"source":"classic_18 = pd.read_csv(r'datasets/classic_18.csv')\nclassic_19 = pd.read_csv(r'datasets/classic_19.csv')\npop_18 = pd.read_csv(r'datasets/pop_18.csv')\npop_19 = pd.read_csv(r'datasets/pop_19.csv')\n\n# Concatenate the classic tables vertically\nclassic_18_19 = pd.concat([classic_18, classic_19], ignore_index = True)\n\n# Concatenate the pop tables vertically\npop_18_19 = pd.concat([pop_18, pop_19], ignore_index = True)\n\n# Merge classic_18_19 with pop_18_19\nclassic_pop = classic_18_19.merge(pop_18_19, on = 'tid', how = 'inner')\n\n# Using .isin(), filter classic_18_19 rows where tid is in classic_pop\npopular_classic = classic_18_19[classic_18_19.tid.isin(classic_pop.tid)]\n\n# Print popular chart\nprint(popular_classic)","metadata":{},"cell_type":"code","id":"a765693a-6503-4ab6-8699-061236197d3b","execution_count":null,"outputs":[]},{"source":"## 4 - Querying and Manipulating DataFrames","metadata":{"tags":[]},"cell_type":"markdown","id":"b8e86414-0f79-49ff-9013-5eaf88986de5"},{"source":"##### Shaping Data with `.melt()`","metadata":{},"cell_type":"markdown","id":"e9ad4efd-7bba-4492-ba0d-6d31f75a3696"},{"source":"ur_wide = ur_wide = pd.read_csv(r'datasets/ur_wide.csv')\n# following 2 lines are required for datacamp code to function\nur_wide.drop('origin_idx', axis = 1, inplace = True)\n\n# unpivot everything besides the year column\nur_tall = ur_wide.melt(id_vars = ['year'], var_name = ['month'], value_name = 'unempl_rate')\n\n# Create a date column using the month and year columns of ur_tall\nur_tall['date'] = pd.to_datetime(ur_tall.year.astype(str) + '-' + ur_tall.month)\n\n# Sort ur_tall by date in ascending order\nur_sorted = ur_tall.sort_values(['date'])\n\n# Plot the unempl_rate by date\nur_sorted.plot(x = 'date', y = 'unempl_rate')\nplt.show()","metadata":{},"cell_type":"code","id":"1f5014cd-b679-406b-b9ed-aaa8c733f524","execution_count":null,"outputs":[]},{"source":"##### Shaping Data with `.melt()` and Filtering with `.query()`","metadata":{},"cell_type":"markdown","id":"e9de8c65-e05d-4e36-85f7-44db507ea5f7"},{"source":"ten_yr = pd.read_csv(r'datasets/ten_yr.csv')\ndji = pd.read_csv(r'datasets/dji.csv')\n\n# Use melt on ten_yr, unpivot everything besides the metric column\nbond_perc = ten_yr.melt(id_vars = ['metric'], var_name = ['date'], value_name = 'close')\n\n# Use query on bond_perc to select only the rows where metric=close\nbond_perc_close = bond_perc.query('metric == \"close\"')\n\n# Merge (ordered) dji and bond_perc_close on date with an inner join\ndow_bond = pd.merge_ordered(dji, bond_perc_close, on = 'date', how = 'inner', suffixes = ('_dow', '_bond'))\n\n# Plot only the close_dow and close_bond columns\ndow_bond.plot(y = ['close_dow', 'close_bond'], x='date', rot=90)\nplt.show()","metadata":{},"cell_type":"code","id":"b6b674f5-f96e-405d-b3d6-3765b692cb69","execution_count":null,"outputs":[]},{"source":"from zip_util import compress_folder, decompress_folder\nimport os, sys\nfrom pathlib import Path\n\ncwd = Path(os.getcwd())\ndatasets = cwd.joinpath('datasets')\narchive = cwd.joinpath('pd_datasets.zip')\ncompress_folder(datasets, archive, True)","metadata":{},"cell_type":"code","id":"82262b2f-0b6d-4965-b141-641bb1d685b5","execution_count":2,"outputs":[{"name":"stdout","text":"Compressing /work/files/workspace/datasets to /work/files/workspace/pd_datasets.zip\nCompressed /work/files/workspace/datasets to /work/files/workspace/pd_datasets.zip\n","output_type":"stream"}]},{"source":"","metadata":{},"cell_type":"code","id":"996d9459-d34c-4399-9f13-48509812b802","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}