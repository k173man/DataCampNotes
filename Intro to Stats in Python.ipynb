{"cells":[{"source":"# Introduction to Statistics in Python","metadata":{},"id":"20c73b4c-626a-4674-b2ca-0f4c47f60628","cell_type":"markdown"},{"source":"## 0 - Setup","metadata":{},"cell_type":"markdown","id":"4e3dabc0-8aa8-4bff-9ab5-1eac23f2d916"},{"source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os, sys\nimport pandas as pd\nfrom scipy.stats import binom, uniform\n\n\n# Importing the course datasets\ndeals = pd.read_csv(\"datasets/amir_deals.csv\")\nhappiness = pd.read_csv(\"datasets/world_happiness.csv\")\nfood_consumption = pd.read_csv(\"datasets/food_consumption.csv\")\nrestaurant_groups = pd.read_csv(\"datasets/restaurant_groups.csv\")\n\nes = lambda n: '=' * n","metadata":{"executionTime":33,"lastSuccessfullyExecutedCode":"# Importing the course packages\nimport numpy as np\nimport pandas as pd\n\n# Importing the course datasets\ndeals = pd.read_csv(\"datasets/amir_deals.csv\")\nhappiness = pd.read_csv(\"datasets/world_happiness.csv\")\nfood = pd.read_csv(\"datasets/food_consumption.csv\")","tags":[]},"id":"cc8f4fbc-8936-468a-b789-fe76aae1fc03","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## 1 - Summary Statistics","metadata":{},"id":"47777ca5-c181-41a3-8eef-b305c567d99d","cell_type":"markdown"},{"source":"##### Measure of Center - Mean and Median - Belgium and USA","metadata":{},"cell_type":"markdown","id":"d7b134e7-41f9-4fdc-a794-6610255dead9"},{"source":"# Subset for Belgium and USA only\nbe_and_usa = food_consumption[(food_consumption.country == 'Belgium') | (food_consumption.country == 'USA')]\n\n# Group by country, select consumption column, and compute mean and median\nprint(be_and_usa.groupby(['country'])['consumption'].agg([np.mean, np.median]))","metadata":{"executionTime":16,"lastSuccessfullyExecutedCode":"# Add your code snippets here"},"id":"fb5e8360-ad35-430d-a8d1-559df0c5baae","cell_type":"code","execution_count":null,"outputs":[]},{"source":"##### Mean vs. Median","metadata":{},"cell_type":"markdown","id":"0d1289b2-32e0-4555-9145-123f109783b1"},{"source":"# Subset for food_category equals rice\nrice_consumption = food_consumption.query('food_category == \"rice\"')\n\nco2_mean_median = rice_consumption.agg({'co2_emission': [np.mean, np.median]})\nco2_mean = co2_mean_median.iat[0, 0]\nco2_median = co2_mean_median.iat[1, 0]\n\n# Histogram of co2_emission for rice and show plot\nrice_consumption.co2_emission.hist()\nplt.vlines(x = co2_mean, ymin = 0, ymax = co2_mean, colors = 'red', label = 'Mean')\nplt.vlines(x = co2_median, ymin = 0, ymax = co2_mean, colors = 'green', label = 'Median')\nplt.show()","metadata":{},"cell_type":"code","id":"6f4dff51-6472-4f2f-be52-7c3784b3ef79","execution_count":null,"outputs":[]},{"source":"##### Quartiles, Quantiles, and Quintiles","metadata":{},"cell_type":"markdown","id":"e02a26f8-ae75-4f74-8c10-94758c855dab"},{"source":"# Calculate the quartiles of co2_emission; both statement produce the same results; for .linspace(...), the 3rd arg is non-inclusive\nprint(np.quantile(food_consumption.co2_emission, np.linspace(0, 1, 5)))\n# print(np.quantile(food_consumption.co2_emission, [0, .25, .5, .75, 1]))","metadata":{},"cell_type":"code","id":"848e49cf-84cb-4e07-9428-83e3898f11a4","execution_count":null,"outputs":[]},{"source":"##### Variance and Standard Deviation","metadata":{},"cell_type":"markdown","id":"3305c842-7bd2-4538-9a30-8ba25ee6cc22"},{"source":"print(f'{es(20)} Variance and Std. Deviation by Food Category {es(20)}')\nprint(food_consumption.groupby('food_category')['co2_emission'].agg([np.var, np.std]))","metadata":{},"cell_type":"code","id":"d7e6bcb2-35f5-4e56-a4d6-a5ce37064356","execution_count":null,"outputs":[]},{"source":"##### Histogram of CO<sub>2</sub> Emissions for Beef and Eggs","metadata":{},"cell_type":"markdown","id":"c660a3da-3703-4096-8b15-506667126dfe"},{"source":"fig, ax = plt.subplots(1, 2)\n\n# 2 lines below filter the co2_emission *Series*\nbeef = food_consumption.co2_emission[food_consumption.food_category == 'beef']\neggs = food_consumption.co2_emission[food_consumption.food_category == 'eggs']\n\n# 2 lines below filter the *DataFrame*, then accesses the co2_emission *Series*\n# beef = food_consumption[food_consumption.food_category == 'beef'].co2_emission.hist()\n# eggs = food_consumption[food_consumption.food_category == 'eggs'].co2_emission\n\n# Create histogram of co2_emission for food_category 'beef'\nax[0].hist(beef)\nax[0].set_title('Beef')\n\n# Create histogram of co2_emission for food_category 'eggs'\nax[1].hist(eggs)\nax[1].set_title('Eggs')\n\nplt.show()","metadata":{},"cell_type":"code","id":"b3f7d04c-5214-41a3-bc54-4cd0d2318792","execution_count":null,"outputs":[]},{"source":"##### Finding Outliers Using IQR","metadata":{},"cell_type":"markdown","id":"13db8d89-d53c-46ca-bf1c-3b8f4046bf50"},{"source":"# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n\n# Compute the first and third quantiles and IQR of emissions_by_country\nq1 = np.quantile(emissions_by_country, 0.25)\nq3 = np.quantile(emissions_by_country, 0.75)\niqr = q3 - q1\n\n# Calculate the lower and upper cutoffs for outliers\nlower = q1 - 1.5 * iqr\nupper = q3 + 1.5 * iqr\n\n# Subset emissions_by_country to find outliers\noutliers = emissions_by_country[(emissions_by_country < lower) | (emissions_by_country > upper)]\nprint(outliers)","metadata":{},"cell_type":"code","id":"2a0d286c-04a1-46ba-905f-173efe359069","execution_count":null,"outputs":[]},{"source":"## 2 - Random Numbers and Probability","metadata":{},"cell_type":"markdown","id":"3f9ec190-24a8-485c-84a8-47f4af14ae54"},{"source":"#### Calculating Probability","metadata":{"tags":[],"jp-MarkdownHeadingCollapsed":true},"cell_type":"markdown","id":"04948abd-3f0f-41d0-bbe0-a462dc5bd89f"},{"source":"##### Get Counts by Product","metadata":{},"cell_type":"markdown","id":"c83ff36b-0609-44dc-aa8d-450693234750"},{"source":"# Count the deals for each product\ncounts = amir_deals['product'].value_counts()\n# produces the same results as previous line; .size() includes NULLs, .count() does not\n# counts = amir_deals.groupby('product').size()","metadata":{},"cell_type":"code","id":"6d2a3828-7c3c-44c4-be84-6e7429760b63","execution_count":null,"outputs":[]},{"source":"##### Calculate Probability","metadata":{},"cell_type":"markdown","id":"709d0606-75ed-4c2a-a085-1d8ef4d881f3"},{"source":"# Calculate probability of picking a deal with each product\nprobs = counts / counts.sum()\nprint(probs)","metadata":{},"cell_type":"code","id":"44dcad1e-a417-4174-a5dc-1b878284fc8e","execution_count":null,"outputs":[]},{"source":"#### Sampling","metadata":{"tags":[]},"cell_type":"markdown","id":"6775495d-c26e-4c0a-a738-6e63574bd596"},{"source":"##### Sampling without Replacement","metadata":{"tags":[]},"cell_type":"markdown","id":"725822b5-1580-44b3-a38a-00082a3baf09"},{"source":"# seeding allows for reproducable results\nnp.random.seed(24)\n# Sample 5 deals without replacement\nsample_without_replacement = amir_deals.sample(5)","metadata":{},"cell_type":"code","id":"c0b2be02-21f0-4149-9349-be51e127f213","execution_count":null,"outputs":[]},{"source":"##### Sampling with Replacement","metadata":{},"cell_type":"markdown","id":"78ee5d46-0bc9-4838-bed7-9751120d3a97"},{"source":"# Sample 5 deals with replacement\nsample_with_replacement = amir_deals.sample(5, replace = True)","metadata":{},"cell_type":"code","id":"18fa69ee-1001-45f5-b9ac-54ae2953a30f","execution_count":null,"outputs":[]},{"source":"#### Probability Distributions","metadata":{},"cell_type":"markdown","id":"1ab760b9-d997-47c9-910c-c2eb54a87e02"},{"source":"##### Discrete Distributions","metadata":{},"cell_type":"markdown","id":"e68df06c-28de-4573-90a6-4e178c20b424"},{"source":"# Create probability distribution\nsize_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n# Reset index and rename columns\nsize_dist = size_dist.reset_index()\nsize_dist.columns = ['group_size', 'prob']\n\n# Expected value\nexpected_value = np.sum(size_dist.group_size * size_dist.prob)\n\n# Subset groups of size 4 or more\ngroups_4_or_more = size_dist[size_dist.group_size >= 4]\n\n# Sum the probabilities of groups_4_or_more\nprob_4_or_more = np.sum(groups_4_or_more.prob)\nprint(prob_4_or_more)","metadata":{},"cell_type":"code","id":"e7ce4013-d29a-43ca-8c94-c93f257f0c87","execution_count":null,"outputs":[]},{"source":"##### Binomial Distribution - Simulating a Sales Deal","metadata":{},"cell_type":"markdown","id":"978a2c05-d7ad-4177-a334-374b9c2316ae"},{"source":"# Set random seed to 10\nnp.random.seed(10)\n\n# Simulate 52 weeks of 3 deals\ndeals = binom.rvs(3, .3, size = 52)\n\n# Print mean deals won per week; min mean == 0, max mean == 3\n# mean/3 == % of deals closed on average\nprint(np.mean(deals))","metadata":{},"cell_type":"code","id":"fa70d27e-8351-4a13-a212-647aee53d738","execution_count":null,"outputs":[]},{"source":"##### Calculating Binomial Probabilities - Closing 3 of 3 Deals","metadata":{"tags":[]},"cell_type":"markdown","id":"a794cee1-5d29-4bac-8ba2-0e583cffb9aa"},{"source":"# Probability of closing 3 out of 3 deals\nprob_3 = binom.pmf(3, 3, .3)\nprint(prob_3)","metadata":{},"cell_type":"code","id":"5082e195-86d7-418a-baef-2c24851f9f38","execution_count":null,"outputs":[]},{"source":"##### Calculating Binomial Probabilities - Closing <= 1 of 3 Deals","metadata":{},"cell_type":"markdown","id":"e637114c-7dc0-4e36-bf0d-817cd90e724b"},{"source":"# Probability of closing <= 1 deal out of 3 deals\nprob_less_than_or_equal_1 = binom.cdf(1, 3, .3)\nprint(prob_less_than_or_equal_1)","metadata":{},"cell_type":"code","id":"c2b5f785-03f9-44b8-aa13-121181cc26b1","execution_count":null,"outputs":[]},{"source":"##### Calculating Binomial Probabilities - Closing > 1 of 3 Deals","metadata":{},"cell_type":"markdown","id":"7e045c8c-a3d1-4605-8250-bc701a979cf9"},{"source":"# Probability of closing > 1 deal out of 3 deals\nprob_greater_than_1 = 1 - binom.cdf(1, 3, .3)\nprint(prob_greater_than_1)","metadata":{},"cell_type":"code","id":"1e94bae8-42a6-436b-9d1d-c6a1f3c3080d","execution_count":null,"outputs":[]},{"source":"##### Continuous Distributions","metadata":{},"cell_type":"markdown","id":"b11b6fca-1e43-4a0f-bd1d-75c44229487e"},{"source":"##### Uniform Distribution - Wait Time <= 5 Minutes","metadata":{},"cell_type":"markdown","id":"e591a4cc-0f10-43e2-93e0-071a4b40dbbc"},{"source":"from scipy.stats import uniform\n\n# Min and max wait times for back-up that happens every 30 min\nmin_time = 0\nmax_time = 30\n\n# Calculate probability of waiting more than 5 mins\nprob_less_than_than_5 = uniform.cdf(5, min_time, max_time)\nprint(prob_greater_than_5)","metadata":{},"cell_type":"code","id":"da7e3bfe-aa56-4063-bed7-3cce24fef5c8","execution_count":null,"outputs":[]},{"source":"##### Uniform Distribution - Wait Time > 5 Minutes","metadata":{"tags":[]},"cell_type":"markdown","id":"db1d9fe3-2c1d-4d7e-bbe3-f6f5f3add7d0"},{"source":"prob_greater_than_5 = 1 - uniform.cdf(5, min_time, max_time)\nprint(prob_greater_than_5)","metadata":{},"cell_type":"code","id":"3212a862-f29a-4470-a0cb-9f4570a9c862","execution_count":null,"outputs":[]},{"source":"##### Uniform Distribution - Wait Time Between 10 and 20 Minutes","metadata":{},"cell_type":"markdown","id":"df8cd45d-1f9c-49d6-889a-882be1e42b3e"},{"source":"# Calculate probability of waiting 10-20 mins\nprob_between_10_and_20 = uniform.cdf(20, min_time, max_time) - uniform.cdf(10, min_time, max_time)\nprint(prob_between_10_and_20)","metadata":{},"cell_type":"code","id":"8878c779-1f56-497b-b2dd-59ab8f859702","execution_count":null,"outputs":[]},{"source":"##### Simulating Wait Times","metadata":{},"cell_type":"markdown","id":"03208967-8e64-4b66-89eb-346c695fff9b"},{"source":"# Set random seed to 334\nnp.random.seed(334)\n\n# Generate 1000 wait times between 0 and 30 mins\nwait_times = uniform.rvs(0, 30, size=1000)\n\n# Create a histogram of simulated times and show plot\nplt.hist(wait_times)\nplt.show()","metadata":{},"cell_type":"code","id":"d9411cad-c5ab-48c0-96f3-2dbf973348be","execution_count":null,"outputs":[]},{"source":"## 3 - More Distributions and the Central Limit Theorem","metadata":{},"cell_type":"markdown","id":"c3ff253b-ad69-4544-97ab-31846147dc24"},{"source":"","metadata":{},"cell_type":"markdown","id":"17130028-7259-43da-a9a2-2d534b1c112b"},{"source":"","metadata":{},"cell_type":"code","id":"8100d005-e9b5-4c52-aaef-dacd693c373d","execution_count":null,"outputs":[]},{"source":"","metadata":{},"cell_type":"code","id":"d05a7eb7-d749-43a3-99bc-01411ccc6375","execution_count":null,"outputs":[]},{"source":"","metadata":{},"cell_type":"code","id":"8a2247f5-18bf-4362-8e4b-49c4fb78ab11","execution_count":null,"outputs":[]},{"source":"","metadata":{},"cell_type":"code","id":"9383bbb7-89c9-4916-83bd-74c77d48f61e","execution_count":null,"outputs":[]},{"source":"","metadata":{},"cell_type":"code","id":"68b643d1-8d46-4626-b248-4b8aca9ad376","execution_count":null,"outputs":[]},{"source":"from zip_util import compress_folder, decompress_folder\nimport os, sys\nfrom pathlib import Path\n\ncwd = Path(os.getcwd())\ndatasets = cwd.joinpath('datasets')\narchive = cwd.joinpath('stats_datasets.zip')\ncompress_folder(datasets, archive, True)","metadata":{},"cell_type":"code","id":"bc52096c-8c11-440a-b52d-6e8536372058","execution_count":1,"outputs":[{"name":"stdout","text":"Compressing /work/files/workspace/datasets to /work/files/workspace/stats_datasets.zip\nCompressed /work/files/workspace/datasets to /work/files/workspace/stats_datasets.zip\n","output_type":"stream"}]},{"source":"","metadata":{},"cell_type":"code","id":"31c12589-1852-4940-8ef5-9988e623f3f6","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}